{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f507cc",
   "metadata": {},
   "source": [
    "# Gambler's problem (Dynamic programming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f87b2c",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a765d1f",
   "metadata": {},
   "source": [
    "*(Based on Example 4.3 in Sutton & Barto)*\n",
    "\n",
    "A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he has staked on that flip; if it is tails, he loses his stake.\n",
    "\n",
    "The game ends when the gambler wins by reaching his goal of $100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP. \n",
    "\n",
    "The state is the gamblerâ€™s capital, $ s \\in\\{1, 2,..., 99\\}$ and the actions are stakes, $a \\in \\{1, 2, ..., min(s, 100- s)\\}$.\n",
    "The reward is zero on all transitions except those on which the gambler reaches his goal, when it is $+1$.\n",
    "Let $p_{h} = 0.4$ denote the probability of the coin coming up heads."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "335e5d83",
   "metadata": {},
   "source": [
    "Assign the values mentioned in the description to the constants below. (*Sidenote: in Python, there are no constants, but by convention, we use all caps for variables that are not supposed to be changed.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b0822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Money goal to reach\n",
    "MAX_MONEY = ??\n",
    "\n",
    "# Possible values of probability of the coin coming up heads\n",
    "P_HEADS =  ??\n",
    "\n",
    "# Small number determining the accuracy of policy evaluation's estimation\n",
    "THETA = 1e-15\n",
    "\n",
    "# Discount factor (can be 1, since this is an episodic task)\n",
    "GAMMA = 1\n",
    "\n",
    "# A list/array of all possible states\n",
    "STATES = np.arange(MAX_MONEY + 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "808b61d2",
   "metadata": {},
   "source": [
    "## 2. Policy Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "558ec238",
   "metadata": {},
   "source": [
    "In this section we evaluate a given deterministic policy $\\pi$ by computing the state-value function $v_{\\pi}$.\n",
    "\n",
    "Define the policy you want to evaluate as an array `policy` of adequate length, where `policy[s]` is the action to take when the gambler's capital is `s`.\n",
    "\n",
    "Initialize the value function as an array `values` of adequate length, where `values[s]` is the value of state `s`. Make sure that terminal states have value zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use length `MAX_MONEY + 1` to include the terminal state\n",
    "policy = ??\n",
    "values = ??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c2589ff",
   "metadata": {},
   "source": [
    "First, we implement a helper function that evaluates an action `a` for a given state `s`, using the current value function `currentValues`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ef0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalAction(s, a, currentValues):\n",
    "    # If we are already in a terminal state, return 0\n",
    "    # ...??\n",
    "    \n",
    "    # Check that s+a and s-a are valid states (optional)\n",
    "    # ...??\n",
    "\n",
    "    # Compute and return the expected reward + value of the next state\n",
    "    # ...??\n",
    "\n",
    "    return # ...??\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf05c493",
   "metadata": {},
   "source": [
    "Implement iterative policy evaluation. You can follow the pseudo-code on page 75 in Sutton & Barto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop:\n",
    "    # delta <- 0\n",
    "\n",
    "    # Loop for each s in S:\n",
    "        # v <- V(s)\n",
    "\n",
    "        # V(s) <- sum_a ...\n",
    "        # (this part might require multiple lines of code)\n",
    "\n",
    "        # delta <- max(delta, |v - V(s)|)\n",
    "\n",
    "    # until delta < THETA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a127c3b7",
   "metadata": {},
   "source": [
    "Plot the value function you computed above.\n",
    "Depending on your implementation, it might look nicer to ignore the terminal states (`0`, `MAX_MONEY`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa71539",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# ...??\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "611ca345",
   "metadata": {},
   "source": [
    "Does the value function look as expected?\n",
    "How does the value function change for different policies, probabilities (`P_HEADS`), and discount factors (`GAMMA`)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f0e5b61",
   "metadata": {},
   "source": [
    "## 3. Policy Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e525645f",
   "metadata": {},
   "source": [
    "In this section, we compute the optimal policy and value function using policy iteration.\n",
    "We implement the policy evaluation and improvement steps as individual functions, which we then call alternately for the actual policy iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e48df",
   "metadata": {},
   "source": [
    "\n",
    "Implement the policy evaluation step. You can use your code from above, but need to wrap it in a function that takes the current policy as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policyEvaluation(policy):\n",
    "\n",
    "    # ...?? (Same code as above)\n",
    "    \n",
    "    return values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "977b91cc",
   "metadata": {},
   "source": [
    "Implement the policy improvement step as a function that takes the current value function as an argument and returns the new (greedy) policy.\n",
    "\n",
    "*Note: In this particular example, there are multiple optimal policies. Your results might look nicer if you break ties (or almost ties) in favor of lower stakes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa149a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyImprovement(values):\n",
    "\n",
    "    # ...??\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "299e5061",
   "metadata": {},
   "source": [
    "Implement policy iteration, using the functions you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d9bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values and policy\n",
    "# ...??\n",
    "\n",
    "# Do policy iteration\n",
    "while True:\n",
    "    # Policy evaluation\n",
    "    # ...??\n",
    "\n",
    "    # Policy improvement\n",
    "    # ...??\n",
    "\n",
    "    # Check if policy has changed\n",
    "    # ...??\n",
    "\n",
    "    # Update policy\n",
    "    # ...??\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "260a694c",
   "metadata": {},
   "source": [
    "Plot the optimal value function and the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a042edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "437ac65f",
   "metadata": {},
   "source": [
    "## 4. Value Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d30453a2",
   "metadata": {},
   "source": [
    "In this section we compute the optimal policy and value function using value iteration.\n",
    "Again, we follow the pseudo-code from Sutton & Barto (p. 83)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values\n",
    "\n",
    "# Loop:\n",
    "    # delta <- 0\n",
    "\n",
    "    # Loop for each s in S:\n",
    "        # v <- V(s)\n",
    "\n",
    "        # V(s) <- max_a sum_s' ...\n",
    "        # (this part might require multiple lines of code)\n",
    "\n",
    "        # delta <- max(delta, |v - V(s)|)\n",
    "\n",
    "    # until delta < THETA\n",
    "\n",
    "# Output a deterministic policy such that\n",
    "# policy[s] = argmax_a ...\n",
    "# (this part might require another loop over the states/actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0d3832",
   "metadata": {},
   "source": [
    "Plot the optimal value function and the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ...??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df9502",
   "metadata": {},
   "source": [
    "### 4.a Bonus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5781f4fd",
   "metadata": {},
   "source": [
    "Find and plot *all* optimal policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7832e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tolerance for \"equal\" values\n",
    "TOL_OPTIMAL = 10 * THETA\n",
    "\n",
    "\n",
    "# Initialize empty policy\n",
    "# This time, this will be a list of lists\n",
    "policy = []\n",
    "\n",
    "# Loop for each state\n",
    "for state in STATES:\n",
    "    # If we are in a terminal state, do nothing\n",
    "    if state == 0 or state == MAX_MONEY:\n",
    "        policy.append([])\n",
    "        continue\n",
    "\n",
    "    # Evaluate all actions\n",
    "    # ...??\n",
    "    \n",
    "    # Choose all optimal actions\n",
    "    # ...??\n",
    "\n",
    "    # Append best actions to policy\n",
    "    # ...??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot with all optimal state/action pairs\n",
    "# ...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
